{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "AiaeQjGVbQPb",
        "outputId": "3821a3d8-f771-41bc-86ba-09025ea9cd0b",
        "collapsed": true
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Initial Value Function:\n",
            "[[ 0.  0.  0.  1.]\n",
            " [ 0. inf  0. -1.]\n",
            " [ 0.  0.  1.  0.]] \n",
            "\n",
            "Iteration 1:\n",
            "[[ 0.   0.   0.9  1. ]\n",
            " [ 0.   inf  0.9 -1. ]\n",
            " [ 0.   0.9  1.   0.9]] \n",
            "\n",
            "Iteration 2:\n",
            "[[ 0.    0.81  0.9   1.  ]\n",
            " [ 0.     inf  0.9  -1.  ]\n",
            " [ 0.81  0.9   1.    0.9 ]] \n",
            "\n",
            "Iteration 3:\n",
            "[[ 0.729  0.81   0.9    1.   ]\n",
            " [ 0.729    inf  0.9   -1.   ]\n",
            " [ 0.81   0.9    1.     0.9  ]] \n",
            "\n",
            "Iteration 4:\n",
            "[[ 0.729  0.81   0.9    1.   ]\n",
            " [ 0.729    inf  0.9   -1.   ]\n",
            " [ 0.81   0.9    1.     0.9  ]] \n",
            "\n",
            "Iteration 5:\n",
            "[[ 0.729  0.81   0.9    1.   ]\n",
            " [ 0.729    inf  0.9   -1.   ]\n",
            " [ 0.81   0.9    1.     0.9  ]] \n",
            "\n",
            "Iteration 6:\n",
            "[[ 0.729  0.81   0.9    1.   ]\n",
            " [ 0.729    inf  0.9   -1.   ]\n",
            " [ 0.81   0.9    1.     0.9  ]] \n",
            "\n"
          ]
        }
      ],
      "source": [
        "import numpy as np\n",
        "\n",
        "matrix = np.array([[0, 0, 0, 1],\n",
        "                   [0, 10, 0, -1],\n",
        "                   [0, 0, 1, 0]])\n",
        "\n",
        "direction = [(-1, 0), (1, 0), (0, -1), (0, 1)]\n",
        "\n",
        "gamma = 0.9\n",
        "iterations = 6\n",
        "\n",
        "rows, cols = matrix.shape\n",
        "\n",
        "infinity = float('inf')\n",
        "new_matrix = np.where(matrix == 10, infinity, matrix).astype(float)\n",
        "\n",
        "print(\"Initial Value Function:\")\n",
        "print(new_matrix, \"\\n\")\n",
        "\n",
        "for it in range(iterations):\n",
        "    temp_matrix = new_matrix.copy()\n",
        "\n",
        "    for row in range(rows):\n",
        "        for col in range(cols):\n",
        "            if matrix[row, col] in {1, -1} or new_matrix[row, col] == infinity:\n",
        "                continue\n",
        "\n",
        "            values = [new_matrix[new_row, new_col] for dr, dc in direction\n",
        "                      if 0 <= (new_row := row + dr) < rows\n",
        "                      and 0 <= (new_col := col + dc) < cols\n",
        "                      and new_matrix[new_row, new_col] != infinity]\n",
        "\n",
        "            if values:\n",
        "                temp_matrix[row, col] = matrix[row, col] + gamma * max(values)\n",
        "\n",
        "    new_matrix = temp_matrix\n",
        "\n",
        "    print(f\"Iteration {it + 1}:\")\n",
        "    print(new_matrix, \"\\n\")\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "\n",
        "# Define a reward matrix\n",
        "reward_matrix = [[0, 0, 0, 1], [0, 0, 0, -1], [0, 0, 0, 0], [0, 0, 0, 0]]\n",
        "\n",
        "# Direction offsets (up, down, left, right)\n",
        "directions = [(-1, 0), (1, 0), (0, -1), (0, 1)]\n",
        "\n",
        "# Set gamma for discounting future rewards\n",
        "gamma = 0.9\n",
        "num_iterations = 5\n",
        "\n",
        "# Get the dimensions of the matrix\n",
        "rows, cols = reward_matrix.shape\n",
        "\n",
        "# Representing infinity for unreachable positions\n",
        "infinity_value = float('inf')\n",
        "modified_matrix = np.where(reward_matrix == 10, infinity_value, reward_matrix).astype(float)\n",
        "\n",
        "# Display the initial value function\n",
        "print(\"Initial Value Function:\")\n",
        "print(modified_matrix, \"\\n\")\n",
        "\n",
        "# Run value iteration for the specified number of iterations\n",
        "for iteration in range(num_iterations):\n",
        "    temp_values = modified_matrix.copy()\n",
        "\n",
        "    for i in range(rows):\n",
        "        for j in range(cols):\n",
        "            # Skip processing if it's a terminal state or already infinity\n",
        "            if reward_matrix[i, j] in {1, -1} or modified_matrix[i, j] == infinity_value:\n",
        "                continue\n",
        "\n",
        "            # List to store the possible values for neighboring states\n",
        "            neighboring_values = []\n",
        "            for di, dj in directions:\n",
        "                # Calculate the new row and column based on direction\n",
        "                new_i, new_j = i + di, j + dj\n",
        "                if 0 <= new_i < rows and 0 <= new_j < cols and modified_matrix[new_i, new_j] != infinity_value:\n",
        "                    neighboring_values.append(modified_matrix[new_i, new_j])\n",
        "\n",
        "            # If valid neighboring states exist, update the value\n",
        "            if neighboring_values:\n",
        "                temp_values[i, j] = reward_matrix[i, j] + gamma * max(neighboring_values)\n",
        "\n",
        "    # Update the matrix for the next iteration\n",
        "    modified_matrix = temp_values\n",
        "\n",
        "    # Print the value function after this iteration\n",
        "    print(f\"After Iteration {iteration + 1}:\")\n",
        "    print(modified_matrix, \"\\n\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "id": "RWrCTf0gfQok",
        "outputId": "ce558637-2d81-4aed-b804-c99388c96c77"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Initial Value Function:\n",
            "[[ 0.  0.  0.  1.]\n",
            " [ 0. inf  0. -1.]\n",
            " [ 0.  0.  1.  0.]] \n",
            "\n",
            "After Iteration 1:\n",
            "[[ 0.   0.   0.9  1. ]\n",
            " [ 0.   inf  0.9 -1. ]\n",
            " [ 0.   0.9  1.   0.9]] \n",
            "\n",
            "After Iteration 2:\n",
            "[[ 0.    0.81  0.9   1.  ]\n",
            " [ 0.     inf  0.9  -1.  ]\n",
            " [ 0.81  0.9   1.    0.9 ]] \n",
            "\n",
            "After Iteration 3:\n",
            "[[ 0.729  0.81   0.9    1.   ]\n",
            " [ 0.729    inf  0.9   -1.   ]\n",
            " [ 0.81   0.9    1.     0.9  ]] \n",
            "\n",
            "After Iteration 4:\n",
            "[[ 0.729  0.81   0.9    1.   ]\n",
            " [ 0.729    inf  0.9   -1.   ]\n",
            " [ 0.81   0.9    1.     0.9  ]] \n",
            "\n",
            "After Iteration 5:\n",
            "[[ 0.729  0.81   0.9    1.   ]\n",
            " [ 0.729    inf  0.9   -1.   ]\n",
            " [ 0.81   0.9    1.     0.9  ]] \n",
            "\n",
            "After Iteration 6:\n",
            "[[ 0.729  0.81   0.9    1.   ]\n",
            " [ 0.729    inf  0.9   -1.   ]\n",
            " [ 0.81   0.9    1.     0.9  ]] \n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "\n",
        "# Given matrix and parameters\n",
        "matrix = np.array([[0, 0, 0, 1],\n",
        "                   [0, 10, 0, -1],\n",
        "                   [0, 0, 1, 0]])\n",
        "\n",
        "directions = [(-1, 0), (1, 0), (0, -1), (0, 1)]\n",
        "gamma = 0.9\n",
        "iterations = 6\n",
        "step_reward = -0.04  # Small negative reward for each step\n",
        "\n",
        "# Initialize value function with zeros\n",
        "value_function = np.zeros(matrix.shape)\n",
        "\n",
        "# Value Iteration\n",
        "for it in range(iterations):\n",
        "    new_value_function = np.copy(value_function)\n",
        "    for i in range(matrix.shape[0]):\n",
        "        for j in range(matrix.shape[1]):\n",
        "            # Skip terminal states, retain their values\n",
        "            if matrix[i][j] == 1 or matrix[i][j] == -1:\n",
        "                new_value_function[i][j] = matrix[i][j]\n",
        "                continue\n",
        "\n",
        "            # Calculate value for each action\n",
        "            values = []\n",
        "            for d in directions:\n",
        "                next_i, next_j = i + d[0], j + d[1]\n",
        "\n",
        "                # Check bounds and calculate the next state value\n",
        "                if 0 <= next_i < matrix.shape[0] and 0 <= next_j < matrix.shape[1]:\n",
        "                    next_value = value_function[next_i][next_j]\n",
        "                else:\n",
        "                    next_value = value_function[i][j]  # If out of bounds, stay in place\n",
        "\n",
        "                # Apply the Bellman update equation\n",
        "                value = step_reward + gamma * next_value\n",
        "                values.append(value)\n",
        "\n",
        "            # Update value function with the maximum value from all actions\n",
        "            new_value_function[i][j] = max(values)\n",
        "\n",
        "    value_function = new_value_function\n",
        "    print(f\"Iteration {it+1}:\\n{value_function}\\n\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "AuoQ7yxjs29y",
        "outputId": "1a0ce4cd-b26b-4138-f1d3-539cc2232d32",
        "collapsed": true
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Iteration 1:\n",
            "[[-0.04 -0.04 -0.04  1.  ]\n",
            " [-0.04 -0.04 -0.04 -1.  ]\n",
            " [-0.04 -0.04  1.   -0.04]]\n",
            "\n",
            "Iteration 2:\n",
            "[[-0.076 -0.076  0.86   1.   ]\n",
            " [-0.076 -0.076  0.86  -1.   ]\n",
            " [-0.076  0.86   1.     0.86 ]]\n",
            "\n",
            "Iteration 3:\n",
            "[[-0.1084  0.734   0.86    1.    ]\n",
            " [-0.1084  0.734   0.86   -1.    ]\n",
            " [ 0.734   0.86    1.      0.86  ]]\n",
            "\n",
            "Iteration 4:\n",
            "[[ 0.6206  0.734   0.86    1.    ]\n",
            " [ 0.6206  0.734   0.86   -1.    ]\n",
            " [ 0.734   0.86    1.      0.86  ]]\n",
            "\n",
            "Iteration 5:\n",
            "[[ 0.6206  0.734   0.86    1.    ]\n",
            " [ 0.6206  0.734   0.86   -1.    ]\n",
            " [ 0.734   0.86    1.      0.86  ]]\n",
            "\n",
            "Iteration 6:\n",
            "[[ 0.6206  0.734   0.86    1.    ]\n",
            " [ 0.6206  0.734   0.86   -1.    ]\n",
            " [ 0.734   0.86    1.      0.86  ]]\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "\n",
        "grid = np.array([[0, 0, 0, 1],\n",
        "                 [0, 2, 0, -1],\n",
        "                 [0, 0, 1, 0]])\n",
        "\n",
        "\n",
        "# States: All grid cells\n",
        "states = [(i, j) for i in range(grid.shape[0]) for j in range(grid.shape[1])]\n",
        "\n",
        "actions = ['up', 'down', 'left', 'right']\n",
        "directions = {\n",
        "    'up': (-1, 0),\n",
        "    'down': (1, 0),\n",
        "    'left': (0, -1),\n",
        "    'right': (0, 1)\n",
        "}\n",
        "\n",
        "\n",
        "gamma = 0.9 # discount factor\n",
        "step_reward = -1  # penalty for each step\n",
        "iterations = 6\n",
        "\n",
        "\n",
        "def transition_model(state, action, next_state):\n",
        "    i, j = state\n",
        "    ni, nj = next_state\n",
        "    if (ni, nj) == (i + directions[action][0], j + directions[action][1]):\n",
        "        # Check bounds\n",
        "        if 0 <= ni < grid.shape[0] and 0 <= nj < grid.shape[1]:\n",
        "            return 1.0\n",
        "    if (ni, nj) == (i, j):  # If out of bounds, stay in place\n",
        "        return 1.0\n",
        "    return 0.0\n",
        "\n",
        "# Reward function\n",
        "def reward_function(state, action, next_state):\n",
        "    ni, nj = next_state\n",
        "    if grid[ni][nj] == 1 or grid[ni][nj] == -1:\n",
        "        return grid[ni][nj]  # Terminal state reward\n",
        "    else:\n",
        "        return step_reward  # Step penalty\n",
        "\n",
        "# Value Iteration for 6 iterations\n",
        "def value_iteration_6(states, actions, transition_model, reward_function, gamma, iterations):\n",
        "    # Initialize value function\n",
        "    V = {s: 0 for s in states}\n",
        "\n",
        "    for it in range(iterations):\n",
        "        new_V = V.copy()\n",
        "        for s in states:\n",
        "            # Skip terminal states\n",
        "            if grid[s[0]][s[1]] in [1, -1]:\n",
        "                new_V[s] = grid[s[0]][s[1]]\n",
        "                continue\n",
        "\n",
        "            # Bellman update\n",
        "            new_V[s] = max(sum(transition_model(s, a, s_next) *\n",
        "                               (reward_function(s, a, s_next) + gamma * V[s_next])\n",
        "                               for s_next in states) for a in actions)\n",
        "\n",
        "        # Update value function\n",
        "        V = new_V\n",
        "\n",
        "        # Display value function after each iteration\n",
        "        print(f\"\\nValue Function after iteration {it+1}:\")\n",
        "        for i in range(grid.shape[0]):\n",
        "            row = \"\"\n",
        "            for j in range(grid.shape[1]):\n",
        "                row += f\"{V[(i, j)]:>8.2f} \"\n",
        "            print(row)\n",
        "\n",
        "# Run value iteration for 6 iterations\n",
        "value_iteration_6(states, actions, transition_model, reward_function, gamma, iterations)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "id": "yZBAkCQFUdU-",
        "outputId": "1b1ecc5a-30a0-4789-8b50-4da3f78adb6d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Value Function after iteration 1:\n",
            "   -1.00    -1.00     0.00     1.00 \n",
            "   -1.00    -2.00     0.00    -1.00 \n",
            "   -1.00     0.00     1.00     0.00 \n",
            "\n",
            "Value Function after iteration 2:\n",
            "   -1.90    -1.90     0.90     1.00 \n",
            "   -1.90    -3.80     0.90    -1.00 \n",
            "   -1.90     0.90     1.00     0.90 \n",
            "\n",
            "Value Function after iteration 3:\n",
            "   -2.71    -2.71     1.71     1.00 \n",
            "   -2.71    -4.61     1.71    -1.00 \n",
            "   -2.71     1.71     1.00     1.71 \n",
            "\n",
            "Value Function after iteration 4:\n",
            "   -3.44    -2.90     2.44     1.00 \n",
            "   -3.44    -4.61     2.44    -1.00 \n",
            "   -2.90     2.44     1.00     2.44 \n",
            "\n",
            "Value Function after iteration 5:\n",
            "   -4.10    -2.41     3.10     1.00 \n",
            "   -4.10    -3.95     3.10    -1.00 \n",
            "   -2.41     3.10     1.00     3.10 \n",
            "\n",
            "Value Function after iteration 6:\n",
            "   -4.69    -1.39     3.69     1.00 \n",
            "   -4.69    -2.77     3.69    -1.00 \n",
            "   -1.39     3.69     1.00     3.69 \n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# main code -\n",
        "import numpy as np\n",
        "\n",
        "grid = np.array([[0, 0, 0, 1],\n",
        "                 [0, 2, 0, -1],\n",
        "                 [0, 0, 1, 0]])\n",
        "\n",
        "# 1: Positive terminal state (reward +1)\n",
        "# -1: Negative terminal state (reward -1)\n",
        "# 2: Wall\n",
        "\n",
        "states = [(i, j) for i in range(grid.shape[0]) for j in range(grid.shape[1])]\n",
        "\n",
        "# Actions: Moving up, down, left, or right\n",
        "actions = ['up', 'down', 'left', 'right']\n",
        "directions = {\n",
        "    'up': (-1, 0),\n",
        "    'down': (1, 0),\n",
        "    'left': (0, -1),\n",
        "    'right': (0, 1)\n",
        "}\n",
        "\n",
        "\n",
        "gamma = 0.99 # discount factor\n",
        "step_reward = -1 # penalty\n",
        "iterations = 6\n",
        "\n",
        "# Initialize Value Function with walls as infinity\n",
        "infinity = float('inf')\n",
        "value_matrix = np.where(grid == 2, infinity, grid).astype(float)\n",
        "\n",
        "print(\"Initial Value Function:\")\n",
        "print(value_matrix, \"\\n\")\n",
        "\n",
        "def transition_model(state, action, next_state):\n",
        "    i, j = state\n",
        "    ni, nj = next_state\n",
        "\n",
        "    # checks if the potential next state (ni, nj) is indeed the state you would end up in by moving from (i, j) according to the specified action.\n",
        "    # If this condition is true it means the action leads directly to the next state (ni, nj)\n",
        "\n",
        "    if (ni, nj) == (i + directions[action][0], j + directions[action][1]):\n",
        "\n",
        "        # Check bounds\n",
        "        if 0 <= ni < grid.shape[0] and 0 <= nj < grid.shape[1]:\n",
        "            # Check for wall, if wall then stay in current state\n",
        "            if grid[ni][nj] == 2:\n",
        "                return 1.0\n",
        "            if (ni, nj) == (i, j)\n",
        "            else 0.0\n",
        "            return 1.0\n",
        "\n",
        "    # If out of bounds or wall, stay in place\n",
        "    if (ni, nj) == (i, j):\n",
        "        return 1.0\n",
        "    return 0.0\n",
        "\n",
        "def reward_function(state, action, next_state):\n",
        "    ni, nj = next_state\n",
        "    if grid[ni][nj] == 1 or grid[ni][nj] == -1:\n",
        "        return grid[ni][nj]  # Terminal state reward\n",
        "    else:\n",
        "        return step_reward\n",
        "\n",
        "def value_iteration_6(states, actions, transition_model, reward_function, gamma, iterations):\n",
        "    V = {s: 0 for s in states}\n",
        "\n",
        "    for it in range(iterations):\n",
        "        new_V = V.copy()\n",
        "        for s in states:\n",
        "            # Skip terminal states and walls\n",
        "            if grid[s[0]][s[1]] in [1, -1] or grid[s[0]][s[1]] == 2:\n",
        "                new_V[s] = grid[s[0]][s[1]]\n",
        "                continue\n",
        "\n",
        "            # Bellman used here\n",
        "            new_V[s] = max(sum(transition_model(s, a, s_next) * (reward_function(s, a, s_next) + gamma * V[s_next])\n",
        "                               for s_next in states) for a in actions)\n",
        "\n",
        "        # Update value function\n",
        "        V = new_V\n",
        "\n",
        "        # Display value function after each iteration\n",
        "        print(f\"\\nValue Function after iteration {it+1}:\")\n",
        "        for i in range(grid.shape[0]):\n",
        "            row = \"\"\n",
        "            for j in range(grid.shape[1]):\n",
        "                if grid[i][j] == 2:\n",
        "                    row += \"  Wall   \"\n",
        "                else:\n",
        "                    row += f\"{V[(i, j)]:>8.2f} \"\n",
        "            print(row)\n",
        "\n",
        "value_iteration_6(states, actions, transition_model, reward_function, gamma, iterations)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rFsXV-TeeTzJ",
        "outputId": "97576968-5ad0-4e73-b112-eaf0b498343f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Initial Value Function:\n",
            "[[ 0.  0.  0.  1.]\n",
            " [ 0. inf  0. -1.]\n",
            " [ 0.  0.  1.  0.]] \n",
            "\n",
            "\n",
            "Value Function after iteration 1:\n",
            "   -1.00    -1.00     0.00     1.00 \n",
            "   -1.00   Wall       0.00    -1.00 \n",
            "   -1.00     0.00     1.00     0.00 \n",
            "\n",
            "Value Function after iteration 2:\n",
            "   -1.99    -1.99     0.99     1.00 \n",
            "   -1.99   Wall       0.99    -1.00 \n",
            "   -1.99     0.99     1.00     0.99 \n",
            "\n",
            "Value Function after iteration 3:\n",
            "   -2.97    -2.97     1.97     1.00 \n",
            "   -2.97   Wall       1.97    -1.00 \n",
            "   -2.97     1.97     1.00     1.97 \n",
            "\n",
            "Value Function after iteration 4:\n",
            "   -3.94    -2.99     2.94     1.00 \n",
            "   -3.94   Wall       2.94    -1.00 \n",
            "   -2.99     2.94     1.00     2.94 \n",
            "\n",
            "Value Function after iteration 5:\n",
            "   -4.90    -2.05     3.90     1.00 \n",
            "   -4.90   Wall       3.90    -1.00 \n",
            "   -2.05     3.90     1.00     3.90 \n",
            "\n",
            "Value Function after iteration 6:\n",
            "   -5.85    -0.17     5.72     1.00 \n",
            "   -5.85   Wall       5.72    -1.00 \n",
            "   -0.17     4.85     1.00     4.85 \n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# main code policy iteration\n",
        "import numpy as np\n",
        "import random\n",
        "\n",
        "# Gridworld setup\n",
        "matrix = np.array([[0, 0, 0, 1],\n",
        "                   [0, 10, 0, -1],\n",
        "                   [0, 0, 1, 0]])\n",
        "\n",
        "direction = {\n",
        "    'up': (-1, 0),\n",
        "    'down': (1, 0),\n",
        "    'left': (0, -1),\n",
        "    'right': (0, 1)\n",
        "}\n",
        "actions = list(direction.keys())\n",
        "\n",
        "gamma = 0.9\n",
        "theta = 1e-4\n",
        "rows, cols = matrix.shape\n",
        "infinity = float('inf')\n",
        "value_matrix = np.where(matrix == 10, infinity, matrix).astype(float)\n",
        "\n",
        "policy = {}\n",
        "for row in range(rows):\n",
        "    for col in range(cols):\n",
        "        if matrix[row, col] not in {1, -1} and matrix[row, col] != 10:\n",
        "            policy[(row, col)] = random.choice(actions)\n",
        "\n",
        "print(\"Initial Policy:\")\n",
        "for r in range(rows):\n",
        "    row = \"\"\n",
        "    for c in range(cols):\n",
        "        if (r, c) in policy:\n",
        "            row += f\"{policy[(r, c)]:>7} \"\n",
        "        elif matrix[r, c] == 10:\n",
        "            row += \"  Wall   \"\n",
        "        else:\n",
        "            row += \"Terminal \"\n",
        "    print(row)\n",
        "print()\n",
        "\n",
        "def get_next_state_and_reward(state, action):\n",
        "    r, c = state\n",
        "    dr, dc = direction[action]\n",
        "    new_r, new_c = r + dr, c + dc\n",
        "\n",
        "    # Check bounds\n",
        "    if 0 <= new_r < rows and 0 <= new_c < cols:\n",
        "        # Check wall\n",
        "        if matrix[new_r, new_c] == 10:\n",
        "            return (r, c), -1  # Stay in place\n",
        "        return (new_r, new_c), matrix[new_r, new_c]\n",
        "    return (r, c), -1  # Out of bounds, stay in place\n",
        "\n",
        "def policy_evaluation(policy, value_matrix, gamma, theta):\n",
        "    while True:\n",
        "        delta = 0\n",
        "        for r in range(rows):\n",
        "            for c in range(cols):\n",
        "                state = (r, c)\n",
        "\n",
        "                if matrix[r, c] in {1, -1} or matrix[r, c] == 10:\n",
        "                    continue  # Skip terminal states and walls\n",
        "\n",
        "                v = value_matrix[r, c]\n",
        "                action = policy[state]\n",
        "\n",
        "                # Get next state and reward\n",
        "                next_state, reward = get_next_state_and_reward(state, action)\n",
        "                value_matrix[r, c] = reward + gamma * value_matrix[next_state]\n",
        "\n",
        "                delta = max(delta, abs(v - value_matrix[r, c]))\n",
        "\n",
        "        # Check for convergence\n",
        "        if delta < theta:\n",
        "            break\n",
        "\n",
        "def policy_improvement(policy, value_matrix, gamma):\n",
        "    policy_stable = True\n",
        "\n",
        "    for r in range(rows):\n",
        "        for c in range(cols):\n",
        "            state = (r, c)\n",
        "\n",
        "            if matrix[r, c] in {1, -1} or matrix[r, c] == 10:\n",
        "                continue  # Skip terminal states and walls\n",
        "\n",
        "            old_action = policy[state]\n",
        "\n",
        "            # Calculate value for all actions\n",
        "            action_values = {}\n",
        "            for action in actions:\n",
        "                next_state, reward = get_next_state_and_reward(state, action)\n",
        "                action_values[action] = reward + gamma * value_matrix[next_state]\n",
        "\n",
        "            # action with the highest value\n",
        "            best_action = max(action_values, key=action_values.get)\n",
        "            policy[state] = best_action\n",
        "\n",
        "            if old_action != best_action:\n",
        "                policy_stable = False\n",
        "\n",
        "    return policy_stable\n",
        "\n",
        "def policy_iteration(policy, value_matrix, gamma, theta):\n",
        "    iteration = 0\n",
        "    while True:\n",
        "        iteration += 1\n",
        "        print(f\"\\nPolicy Iteration {iteration}: Policy Evaluation\")\n",
        "        policy_evaluation(policy, value_matrix, gamma, theta)\n",
        "\n",
        "        print(\"\\nValue Function:\")\n",
        "        for i in range(rows):\n",
        "            row = \"\"\n",
        "            for j in range(cols):\n",
        "                if matrix[i][j] == 10:\n",
        "                    row += \"  Wall   \"\n",
        "                else:\n",
        "                    row += f\"{value_matrix[i][j]:>8.2f} \"\n",
        "            print(row)\n",
        "\n",
        "        print(\"\\nPolicy:\")\n",
        "        for i in range(rows):\n",
        "            row = \"\"\n",
        "            for j in range(cols):\n",
        "                if (i, j) in policy:\n",
        "                    row += f\"{policy[(i, j)]:>8} \"\n",
        "                elif matrix[i][j] == 10:\n",
        "                    row += \"  Wall   \"\n",
        "                else:\n",
        "                    row += \"Terminal \"\n",
        "            print(row)\n",
        "\n",
        "        print(\"\\nPolicy Improvement\")\n",
        "        policy_stable = policy_improvement(policy, value_matrix, gamma)\n",
        "\n",
        "        if policy_stable:\n",
        "            print(\"\\nPolicy Converged!\")\n",
        "            break\n",
        "\n",
        "policy_iteration(policy, value_matrix, gamma, theta)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dts-fiyjAlC7",
        "outputId": "44b2c989-a69a-43d6-9583-f84db14cf15a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Initial Policy:\n",
            "     up      up    left Terminal \n",
            "     up   Wall        up Terminal \n",
            "   left      up Terminal      up \n",
            "\n",
            "\n",
            "Policy Iteration 1: Policy Evaluation\n",
            "\n",
            "Value Function:\n",
            "  -10.00   -10.00    -9.00     1.00 \n",
            "   -9.00   Wall      -8.10    -1.00 \n",
            "  -10.00   -10.00     1.00    -1.90 \n",
            "\n",
            "Policy:\n",
            "      up       up     left Terminal \n",
            "      up   Wall         up Terminal \n",
            "    left       up Terminal       up \n",
            "\n",
            "Policy Improvement\n",
            "\n",
            "Policy Iteration 2: Policy Evaluation\n",
            "\n",
            "Value Function:\n",
            "   -0.00     1.71     1.90     1.00 \n",
            "   -0.00   Wall       1.90    -1.00 \n",
            "   -0.00     1.90     1.00     1.90 \n",
            "\n",
            "Policy:\n",
            "    down    right    right Terminal \n",
            "      up   Wall       down Terminal \n",
            "      up    right Terminal     left \n",
            "\n",
            "Policy Improvement\n",
            "\n",
            "Policy Iteration 3: Policy Evaluation\n",
            "\n",
            "Value Function:\n",
            "    1.54     1.71     1.90     1.00 \n",
            "    1.54   Wall       1.90    -1.00 \n",
            "    1.71     1.90     1.00     1.90 \n",
            "\n",
            "Policy:\n",
            "   right    right    right Terminal \n",
            "    down   Wall       down Terminal \n",
            "   right    right Terminal     left \n",
            "\n",
            "Policy Improvement\n",
            "\n",
            "Policy Converged!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "\n",
        "# 2 = Wall, 1 = Goal, 0 = Open Space\n",
        "maze = np.array([[2, 2, 2, 2, 2, 2, 2, 2, 2, 2],\n",
        "                 [2, 0, 2, 0, 0, 0, 2, 0, 2, 2],\n",
        "                 [2, 0, 2, 0, 2, 2, 2, 0, 1, 2],\n",
        "                 [2, 0, 2, 0, 0, 2, 2, 2, 2, 2],\n",
        "                 [2, 0, 2, 2, 0, 2, 0, 0, 0, 2],\n",
        "                 [2, 0, 0, 0, 0, 2, 2, 0, 2, 2],\n",
        "                 [2, 2, 0, 2, 2, 0, 2, 0, 2, 2],\n",
        "                 [2, 2, 0, 0, 0, 0, 2, 0, 2, 2],\n",
        "                 [2, 2, 2, 2, 2, 0, 0, 0, 2, 2],\n",
        "                 [2, 2, 2, 2, 2, 2, 2, 2, 2, 2]])\n",
        "\n",
        "states = [(i, j) for i in range(maze.shape[0]) for j in range(maze.shape[1])]\n",
        "actions = ['up', 'down', 'left', 'right']\n",
        "directions = {\n",
        "    'up': (-1, 0),\n",
        "    'down': (1, 0),\n",
        "    'left': (0, -1),\n",
        "    'right': (0, 1)\n",
        "}\n",
        "\n",
        "gamma = 0.99\n",
        "step_reward = -1\n",
        "iterations = 6\n",
        "\n",
        "infinity = float('inf')\n",
        "value_matrix = np.where(maze == 2, infinity, maze).astype(float)\n",
        "\n",
        "print(\"Initial Value Function:\")\n",
        "print(value_matrix, \"\\n\")\n",
        "\n",
        "def transition_model(state, action, next_state):\n",
        "    i, j = state\n",
        "    ni, nj = next_state\n",
        "\n",
        "    if (ni, nj) == (i + directions[action][0], j + directions[action][1]):\n",
        "        if 0 <= ni < maze.shape[0] and 0 <= nj < maze.shape[1]:\n",
        "            if maze[ni][nj] == 2:\n",
        "                return 1.0 if (ni, nj) == (i, j) else 0.0\n",
        "            return 1.0\n",
        "\n",
        "    if (ni, nj) == (i, j):\n",
        "        return 1.0\n",
        "    return 0.0\n",
        "\n",
        "def reward_function(state, action, next_state):\n",
        "    ni, nj = next_state\n",
        "    if maze[ni][nj] == 1:\n",
        "        return 1\n",
        "    else:\n",
        "        return step_reward\n",
        "\n",
        "def value_iteration(states, actions, transition_model, reward_function, gamma, iterations):\n",
        "    V = {s: 0 for s in states}\n",
        "\n",
        "    for it in range(iterations):\n",
        "        new_V = V.copy()\n",
        "        for s in states:\n",
        "            if maze[s[0]][s[1]] == 1 or maze[s[0]][s[1]] == 2:\n",
        "                new_V[s] = maze[s[0]][s[1]]\n",
        "                continue\n",
        "\n",
        "            new_V[s] = max(sum(transition_model(s, a, s_next) *\n",
        "                               (reward_function(s, a, s_next) + gamma * V[s_next])\n",
        "                               for s_next in states) for a in actions)\n",
        "\n",
        "        V = new_V\n",
        "\n",
        "        print(f\"\\nValue Function after iteration {it+1}:\")\n",
        "        for i in range(maze.shape[0]):\n",
        "            row = \"\"\n",
        "            for j in range(maze.shape[1]):\n",
        "                if maze[i][j] == 2:\n",
        "                    row += \"  Wall   \"\n",
        "                elif maze[i][j] == 1:\n",
        "                    row += \"  Goal   \"\n",
        "                else:\n",
        "                    row += f\"{V[(i, j)]:>8.2f} \"\n",
        "            print(row)\n",
        "\n",
        "value_iteration(states, actions, transition_model, reward_function, gamma, iterations)\n"
      ],
      "metadata": {
        "id": "m19c33Duc-G3",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "429632e6-060a-42d6-e0f9-50196869f7fc"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Initial Value Function:\n",
            "[[inf inf inf inf inf inf inf inf inf inf]\n",
            " [inf  0. inf  0.  0.  0. inf  0. inf inf]\n",
            " [inf  0. inf  0. inf inf inf  0.  1. inf]\n",
            " [inf  0. inf  0.  0. inf inf inf inf inf]\n",
            " [inf  0. inf inf  0. inf  0.  0.  0. inf]\n",
            " [inf  0.  0.  0.  0. inf inf  0. inf inf]\n",
            " [inf inf  0. inf inf  0. inf  0. inf inf]\n",
            " [inf inf  0.  0.  0.  0. inf  0. inf inf]\n",
            " [inf inf inf inf inf  0.  0.  0. inf inf]\n",
            " [inf inf inf inf inf inf inf inf inf inf]] \n",
            "\n",
            "\n",
            "Value Function after iteration 1:\n",
            "  Wall     Wall     Wall     Wall     Wall     Wall     Wall     Wall     Wall     Wall   \n",
            "  Wall      -1.00   Wall      -1.00    -1.00    -1.00   Wall      -1.00   Wall     Wall   \n",
            "  Wall      -1.00   Wall      -1.00   Wall     Wall     Wall       0.00   Goal     Wall   \n",
            "  Wall      -1.00   Wall      -1.00    -1.00   Wall     Wall     Wall     Wall     Wall   \n",
            "  Wall      -1.00   Wall     Wall      -1.00   Wall      -1.00    -1.00    -1.00   Wall   \n",
            "  Wall      -1.00    -1.00    -1.00    -1.00   Wall     Wall      -1.00   Wall     Wall   \n",
            "  Wall     Wall      -1.00   Wall     Wall      -1.00   Wall      -1.00   Wall     Wall   \n",
            "  Wall     Wall      -1.00    -1.00    -1.00    -1.00   Wall      -1.00   Wall     Wall   \n",
            "  Wall     Wall     Wall     Wall     Wall      -1.00    -1.00    -1.00   Wall     Wall   \n",
            "  Wall     Wall     Wall     Wall     Wall     Wall     Wall     Wall     Wall     Wall   \n",
            "\n",
            "Value Function after iteration 2:\n",
            "  Wall     Wall     Wall     Wall     Wall     Wall     Wall     Wall     Wall     Wall   \n",
            "  Wall      -1.99   Wall      -1.99    -1.99    -1.99   Wall      -1.99   Wall     Wall   \n",
            "  Wall      -1.99   Wall      -1.99   Wall     Wall     Wall       0.99   Goal     Wall   \n",
            "  Wall      -1.99   Wall      -1.99    -1.99   Wall     Wall     Wall     Wall     Wall   \n",
            "  Wall      -1.99   Wall     Wall      -1.99   Wall      -1.99    -1.99    -1.99   Wall   \n",
            "  Wall      -1.99    -1.99    -1.99    -1.99   Wall     Wall      -1.99   Wall     Wall   \n",
            "  Wall     Wall      -1.99   Wall     Wall      -1.99   Wall      -1.99   Wall     Wall   \n",
            "  Wall     Wall      -1.99    -1.99    -1.99    -1.99   Wall      -1.99   Wall     Wall   \n",
            "  Wall     Wall     Wall     Wall     Wall      -1.99    -1.99    -1.99   Wall     Wall   \n",
            "  Wall     Wall     Wall     Wall     Wall     Wall     Wall     Wall     Wall     Wall   \n",
            "\n",
            "Value Function after iteration 3:\n",
            "  Wall     Wall     Wall     Wall     Wall     Wall     Wall     Wall     Wall     Wall   \n",
            "  Wall      -2.97   Wall      -2.97    -2.97    -2.97   Wall      -2.97   Wall     Wall   \n",
            "  Wall      -2.97   Wall      -2.97   Wall     Wall     Wall       1.97   Goal     Wall   \n",
            "  Wall      -2.97   Wall      -2.97    -2.97   Wall     Wall     Wall     Wall     Wall   \n",
            "  Wall      -2.97   Wall     Wall      -2.97   Wall      -2.97    -2.97    -2.97   Wall   \n",
            "  Wall      -2.97    -2.97    -2.97    -2.97   Wall     Wall      -2.97   Wall     Wall   \n",
            "  Wall     Wall      -2.97   Wall     Wall      -2.97   Wall      -2.97   Wall     Wall   \n",
            "  Wall     Wall      -2.97    -2.97    -2.97    -2.97   Wall      -2.97   Wall     Wall   \n",
            "  Wall     Wall     Wall     Wall     Wall      -2.97    -2.97    -2.97   Wall     Wall   \n",
            "  Wall     Wall     Wall     Wall     Wall     Wall     Wall     Wall     Wall     Wall   \n",
            "\n",
            "Value Function after iteration 4:\n",
            "  Wall     Wall     Wall     Wall     Wall     Wall     Wall     Wall     Wall     Wall   \n",
            "  Wall      -3.94   Wall      -3.94    -3.94    -3.94   Wall      -2.99   Wall     Wall   \n",
            "  Wall      -3.94   Wall      -3.94   Wall     Wall     Wall       2.94   Goal     Wall   \n",
            "  Wall      -3.94   Wall      -3.94    -3.94   Wall     Wall     Wall     Wall     Wall   \n",
            "  Wall      -3.94   Wall     Wall      -3.94   Wall      -3.94    -3.94    -3.94   Wall   \n",
            "  Wall      -3.94    -3.94    -3.94    -3.94   Wall     Wall      -3.94   Wall     Wall   \n",
            "  Wall     Wall      -3.94   Wall     Wall      -3.94   Wall      -3.94   Wall     Wall   \n",
            "  Wall     Wall      -3.94    -3.94    -3.94    -3.94   Wall      -3.94   Wall     Wall   \n",
            "  Wall     Wall     Wall     Wall     Wall      -3.94    -3.94    -3.94   Wall     Wall   \n",
            "  Wall     Wall     Wall     Wall     Wall     Wall     Wall     Wall     Wall     Wall   \n",
            "\n",
            "Value Function after iteration 5:\n",
            "  Wall     Wall     Wall     Wall     Wall     Wall     Wall     Wall     Wall     Wall   \n",
            "  Wall      -4.90   Wall      -4.90    -4.90    -4.90   Wall      -2.05   Wall     Wall   \n",
            "  Wall      -4.90   Wall      -4.90   Wall     Wall     Wall       3.90   Goal     Wall   \n",
            "  Wall      -4.90   Wall      -4.90    -4.90   Wall     Wall     Wall     Wall     Wall   \n",
            "  Wall      -4.90   Wall     Wall      -4.90   Wall      -4.90    -4.90    -4.90   Wall   \n",
            "  Wall      -4.90    -4.90    -4.90    -4.90   Wall     Wall      -4.90   Wall     Wall   \n",
            "  Wall     Wall      -4.90   Wall     Wall      -4.90   Wall      -4.90   Wall     Wall   \n",
            "  Wall     Wall      -4.90    -4.90    -4.90    -4.90   Wall      -4.90   Wall     Wall   \n",
            "  Wall     Wall     Wall     Wall     Wall      -4.90    -4.90    -4.90   Wall     Wall   \n",
            "  Wall     Wall     Wall     Wall     Wall     Wall     Wall     Wall     Wall     Wall   \n",
            "\n",
            "Value Function after iteration 6:\n",
            "  Wall     Wall     Wall     Wall     Wall     Wall     Wall     Wall     Wall     Wall   \n",
            "  Wall      -5.85   Wall      -5.85    -5.85    -5.85   Wall      -0.17   Wall     Wall   \n",
            "  Wall      -5.85   Wall      -5.85   Wall     Wall     Wall       4.85   Goal     Wall   \n",
            "  Wall      -5.85   Wall      -5.85    -5.85   Wall     Wall     Wall     Wall     Wall   \n",
            "  Wall      -5.85   Wall     Wall      -5.85   Wall      -5.85    -5.85    -5.85   Wall   \n",
            "  Wall      -5.85    -5.85    -5.85    -5.85   Wall     Wall      -5.85   Wall     Wall   \n",
            "  Wall     Wall      -5.85   Wall     Wall      -5.85   Wall      -5.85   Wall     Wall   \n",
            "  Wall     Wall      -5.85    -5.85    -5.85    -5.85   Wall      -5.85   Wall     Wall   \n",
            "  Wall     Wall     Wall     Wall     Wall      -5.85    -5.85    -5.85   Wall     Wall   \n",
            "  Wall     Wall     Wall     Wall     Wall     Wall     Wall     Wall     Wall     Wall   \n"
          ]
        }
      ]
    }
  ]
}